{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JrH4zTiCw67v"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "from keras.utils import pad_sequences #Use keras.utils.pad_sequences instead of keras.preprocessing.sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer #Import Tokenizer from tensorflow.keras.preprocessing.text\n",
        "import nltk\n",
        "\n",
        "# Step 1: Load Data\n",
        "data = pd.read_csv('cleaned_youtube_comments.csv')  # Replace with your actual file path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nltk' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#import nltk\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvader_lexicon\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "eweMwxpInuhI",
        "outputId": "ab2efbb7-df04-45ab-df2d-c8b6f3372e03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Comments    40\n",
              "dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5wheKReqjVa",
        "outputId": "89bf0647-efca-4d0f-c2d9-a61bbb2a0b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values before removing: 40\n",
            "Null values after removing: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for null values before removing\n",
        "print(f\"Null values before removing: {data['Comments'].isnull().sum()}\")\n",
        "\n",
        "# Remove rows with null values in the 'Comments' column\n",
        "data.dropna(subset=['Comments'], inplace=True)\n",
        "\n",
        "# Check for null values after removing\n",
        "print(f\"Null values after removing: {data['Comments'].isnull().sum()}\")\n",
        "\n",
        "# Save the data without null values back to CSV\n",
        "#data.to_csv('/content/sample_data/cleaned_youtube_comments_without_nulls.csv', index=False)\n",
        "\n",
        "#print(\"Data without null values has been saved to 'cleaned_youtube_comments_without_nulls.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8a8Tf9pobJN",
        "outputId": "de0114a5-2ac1-452f-9a2a-503c02e1621f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Columns: Index(['https://bit.ly/30JSSPr Geo is the hub for all your Pakistani entertainment needs! Hit the bell icon and subscribe to become a part of our growing community, now hitting the USA'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Check existing columns\n",
        "print(\"Original Columns:\", data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2k5tGdbotnA",
        "outputId": "beb8c322-f7ca-47a6-b670-98af04ba53a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original column names: Index(['https://bit.ly/30JSSPr Geo is the hub for all your Pakistani entertainment needs! Hit the bell icon and subscribe to become a part of our growing community, now hitting the USA'], dtype='object')\n",
            "Updated column names: Index(['Comments'], dtype='object')\n",
            "                                            Comments\n",
            "0  People slit their wrists when stupidly crazy i...\n",
            "1  Wahaj ali and yumna jaldi ki cute jodi  hain‚ù§‚ù§...\n",
            "2  Fucking drama,\\nShe Don't know muntasir loves ...\n",
            "3  Jis aurat mein inkar ki jarurat nhi hoti vo au...\n",
            "4                         Kon kon dubara dekh rha hüòÇ\n"
          ]
        }
      ],
      "source": [
        "# Check current column name (to verify)\n",
        "print(\"Original column names:\", data.columns)\n",
        "\n",
        "# Rename the column (replace the long name with 'Comments')\n",
        "data.rename(columns={'https://bit.ly/30JSSPr Geo is the hub for all your Pakistani entertainment needs! Hit the bell icon and subscribe to become a part of our growing community, now hitting the USA': 'Comments'}, inplace=True)\n",
        "\n",
        "# Print the updated DataFrame to confirm the column name change\n",
        "print(\"Updated column names:\", data.columns)\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfmaQoROpmTm",
        "outputId": "e9f8a8d3-5240-4a21-e834-e5f83598a4d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            Comments\n",
            "0  People slit their wrists when stupidly crazy i...\n",
            "1  Wahaj ali and yumna jaldi ki cute jodi  hain‚ù§‚ù§...\n",
            "2  Fucking drama,\\nShe Don't know muntasir loves ...\n",
            "3  Jis aurat mein inkar ki jarurat nhi hoti vo au...\n",
            "4                         Kon kon dubara dekh rha hüòÇ\n"
          ]
        }
      ],
      "source": [
        "# Remove the specific URL and message from the 'Comments' column\n",
        "data['Comments'] = data['Comments'].str.replace(r'https://bit.ly/30JSSPr Geo is the hub for all your Pakistani entertainment needs! Hit the bell icon and subscribe to become a part of our growing community, now hitting the USA', '', regex=True)\n",
        "\n",
        "# Print the cleaned data to confirm\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ESPpl9pp3Fj",
        "outputId": "2a84a908-973e-4d5b-d545-3bd800c96783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned data has been saved to 'cleaned_youtube_comments.csv'.\n"
          ]
        }
      ],
      "source": [
        "# Save the cleaned data back to a CSV file\n",
        "data.to_csv('/content/sample_data/cleaned_youtube_comments.csv', index=False)\n",
        "\n",
        "print(\"Cleaned data has been saved to 'cleaned_youtube_comments.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "JwMw9Zx0rBxG",
        "outputId": "5ae48660-86d7-44df-cb2c-b529ff6462e8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>People slit their wrists when stupidly crazy i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Wahaj ali and yumna jaldi ki cute jodi  hain‚ù§‚ù§...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Fucking drama,\\nShe Don't know muntasir loves ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jis aurat mein inkar ki jarurat nhi hoti vo au...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Kon kon dubara dekh rha hüòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154249</th>\n",
              "      <td>Lo ji Zafar saab... Am waiting from India üòä</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154250</th>\n",
              "      <td>‚ù§Ô∏è‚ù§Ô∏è</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154251</th>\n",
              "      <td>Waiting waiting waiting üòÉ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154252</th>\n",
              "      <td>yeah kre ga dhamaka inshaAllah trending hai. y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154253</th>\n",
              "      <td>First like Maine Kiya ..love u Ali Zafar from ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>154214 rows √ó 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Comments\n",
              "0       People slit their wrists when stupidly crazy i...\n",
              "1       Wahaj ali and yumna jaldi ki cute jodi  hain‚ù§‚ù§...\n",
              "2       Fucking drama,\\nShe Don't know muntasir loves ...\n",
              "3       Jis aurat mein inkar ki jarurat nhi hoti vo au...\n",
              "4                              Kon kon dubara dekh rha hüòÇ\n",
              "...                                                   ...\n",
              "154249        Lo ji Zafar saab... Am waiting from India üòä\n",
              "154250                                               ‚ù§Ô∏è‚ù§Ô∏è\n",
              "154251                          Waiting waiting waiting üòÉ\n",
              "154252  yeah kre ga dhamaka inshaAllah trending hai. y...\n",
              "154253  First like Maine Kiya ..love u Ali Zafar from ...\n",
              "\n",
              "[154214 rows x 1 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ult3O-OIngrm"
      },
      "outputs": [],
      "source": [
        "data['Comments'] = data['Comments'].astype(str)  # Ensure comments are strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZYC62ImHxDRd"
      },
      "outputs": [],
      "source": [
        "data1 = pd.DataFrame (data, columns = ['Comment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5U-W8ZUx_Y2",
        "outputId": "62b03a1c-1a08-45df-bdaa-1e0d4d5348d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\Aleena\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('vader_lexicon')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Text Preprocessing\n",
        "def text_processing(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'@\\w+|#\\w+|http\\S+', '', text)  # Remove mentions, hashtags, and URLs\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), \"\", text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "data[\"Processed_Comment\"] = data[\"Comments\"].apply(text_processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Check Sentiment Counts\n",
        "sentiments = SentimentIntensityAnalyzer()\n",
        "data['Sentiment_Score'] = data['Processed_Comment'].apply(lambda x: sentiments.polarity_scores(x)['compound'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment Counts:\n",
            "Sentiment\n",
            "Neutral     81979\n",
            "Positive    63979\n",
            "Negative     8256\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Categorizing Sentiments\n",
        "data['Sentiment'] = np.where(data['Sentiment_Score'] >= 0.05, 'Positive',\n",
        "                             np.where(data['Sentiment_Score'] <= -0.05, 'Negative', 'Neutral'))\n",
        "\n",
        "# Display sentiment counts\n",
        "sentiment_counts = data['Sentiment'].value_counts()\n",
        "print(\"Sentiment Counts:\")\n",
        "print(sentiment_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Balanced Sentiment Counts:\n",
            "Sentiment\n",
            "Positive    63979\n",
            "Negative    63979\n",
            "Neutral     63979\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Sampling to Balance Data\n",
        "df_positive = data[data['Sentiment'] == 'Positive']\n",
        "df_negative = data[data['Sentiment'] == 'Negative']\n",
        "df_neutral = data[data['Sentiment'] == 'Neutral']\n",
        "\n",
        "# Upsample minority classes\n",
        "df_negative_upsampled = resample(df_negative, replace=True, n_samples=len(df_positive), random_state=42)\n",
        "df_neutral_upsampled = resample(df_neutral, replace=True, n_samples=len(df_positive), random_state=42)\n",
        "\n",
        "# Combine majority class with upsampled minority classes\n",
        "balanced_data = pd.concat([df_positive, df_negative_upsampled, df_neutral_upsampled])\n",
        "\n",
        "# Display balanced sentiment counts\n",
        "balanced_sentiment_counts = balanced_data['Sentiment'].value_counts()\n",
        "print(\"Balanced Sentiment Counts:\")\n",
        "print(balanced_sentiment_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Tokenization and Padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(balanced_data[\"Processed_Comment\"])\n",
        "X = tokenizer.texts_to_sequences(balanced_data[\"Processed_Comment\"])\n",
        "\n",
        "max_len = 100  # Maximum length of sequences\n",
        "X_pad = pad_sequences(X, maxlen=max_len)\n",
        "y = balanced_data['Sentiment'].map({'Positive': 2, 'Negative': 0, 'Neutral': 1}).values  # Mapping labels to integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Build the LSTM Model\n",
        "# Define your model\n",
        "model = Sequential()\n",
        "\n",
        "# Updated Embedding layer without `input_length`\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128))  # Removed `input_length`\n",
        "\n",
        "# Other layers\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "  # Adjust based on the number of sentiment classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:From c:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "4199/4199 [==============================] - 2874s 680ms/step - loss: 0.1263 - accuracy: 0.9560 - val_loss: 0.0430 - val_accuracy: 0.9886\n",
            "Epoch 2/5\n",
            "4199/4199 [==============================] - 4137s 985ms/step - loss: 0.0206 - accuracy: 0.9938 - val_loss: 0.0364 - val_accuracy: 0.9914\n",
            "Epoch 3/5\n",
            "4199/4199 [==============================] - 3995s 951ms/step - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.0396 - val_accuracy: 0.9922\n",
            "Epoch 4/5\n",
            "4199/4199 [==============================] - 3223s 768ms/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0445 - val_accuracy: 0.9912\n",
            "Epoch 5/5\n",
            "4199/4199 [==============================] - 3972s 946ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0482 - val_accuracy: 0.9910\n",
            "1800/1800 [==============================] - 226s 125ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     19139\n",
            "           1       0.99      0.99      0.99     19393\n",
            "           2       0.99      0.99      0.99     19050\n",
            "\n",
            "    accuracy                           0.99     57582\n",
            "   macro avg       0.99      0.99      0.99     57582\n",
            "weighted avg       0.99      0.99      0.99     57582\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 8: Train the Model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 9: Evaluate the Model\n",
        "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I08g08VTwVhW",
        "outputId": "007beca5-a4e8-49ad-bff8-628068d800a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Optional: Save the Model\n",
        "model.save('sentimentk.keras')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optional: Save the Model\n",
        "model.save('sentimentk.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSdLR5kpIbiC",
        "outputId": "44af80f3-2780-4312-fa04-d8d4dd549eeb"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Error when deserializing class 'InputLayer' using config={'batch_shape': [None, 100], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}.\n\nException encountered: Unrecognized keyword arguments: ['batch_shape']",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\engine\\base_layer.py:868\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    869\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\engine\\input_layer.py:153\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[1;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, ragged, type_spec, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m     )\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse \u001b[38;5;129;01mand\u001b[39;00m ragged:\n",
            "\u001b[1;31mValueError\u001b[0m: Unrecognized keyword arguments: ['batch_shape']",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment_model.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming you have a tokenizer saved, load it (you may need to implement this part)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# tokenizer = load_tokenizer('path_to_your_tokenizer.pkl')\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Sample new comments for prediction\u001b[39;00m\n\u001b[0;32m     13\u001b[0m new_comments \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love this product! It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms fantastic!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the worst experience I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve ever had.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms okay, neither good nor bad.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:254\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following argument(s) are not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith the native Keras format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m         )\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    263\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    264\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:281\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    278\u001b[0m             asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[1;32mc:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:246\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[1;32m--> 246\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
            "File \u001b[1;32mc:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:728\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m safe_mode_scope \u001b[38;5;241m=\u001b[39m SafeModeScope(safe_mode)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m--> 728\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m     build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m build_config:\n",
            "File \u001b[1;32mc:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\engine\\sequential.py:466\u001b[0m, in \u001b[0;36mSequential.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_config \u001b[38;5;129;01min\u001b[39;00m layer_configs:\n\u001b[0;32m    465\u001b[0m     use_legacy_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m layer_config\n\u001b[1;32m--> 466\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_legacy_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(layer)\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39minputs\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m build_input_shape\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(build_input_shape, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))\n\u001b[0;32m    477\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\layers\\serialization.py:276\u001b[0m, in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects, use_legacy_format)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_legacy_format:\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_serialization\u001b[38;5;241m.\u001b[39mdeserialize_keras_object(\n\u001b[0;32m    270\u001b[0m         config,\n\u001b[0;32m    271\u001b[0m         module_objects\u001b[38;5;241m=\u001b[39mLOCAL\u001b[38;5;241m.\u001b[39mALL_OBJECTS,\n\u001b[0;32m    272\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    273\u001b[0m         printable_module_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    274\u001b[0m     )\n\u001b[1;32m--> 276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:609\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module_objects[config], types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[0;32m    603\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_keras_object(\n\u001b[0;32m    604\u001b[0m                 serialize_with_public_fn(\n\u001b[0;32m    605\u001b[0m                     module_objects[config], config, fn_module_name\n\u001b[0;32m    606\u001b[0m                 ),\n\u001b[0;32m    607\u001b[0m                 custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    608\u001b[0m             )\n\u001b[1;32m--> 609\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m            \u001b[49m\u001b[43mserialize_with_public_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_config\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PLAIN_TYPES):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
            "File \u001b[1;32mc:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:728\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m safe_mode_scope \u001b[38;5;241m=\u001b[39m SafeModeScope(safe_mode)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m--> 728\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m     build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m build_config:\n",
            "File \u001b[1;32mc:\\Users\\Aleena\\AppData\\Local\\miniconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\engine\\base_layer.py:870\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    869\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when deserializing class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    873\u001b[0m     )\n",
            "\u001b[1;31mTypeError\u001b[0m: Error when deserializing class 'InputLayer' using config={'batch_shape': [None, 100], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}.\n\nException encountered: Unrecognized keyword arguments: ['batch_shape']"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load the model\n",
        "model = load_model('sentiment_model.keras')\n",
        "\n",
        "# Assuming you have a tokenizer saved, load it (you may need to implement this part)\n",
        "# tokenizer = load_tokenizer('path_to_your_tokenizer.pkl')\n",
        "\n",
        "# Sample new comments for prediction\n",
        "new_comments = [\n",
        "    \"I love this product! It's fantastic!\",\n",
        "    \"This is the worst experience I've ever had.\",\n",
        "    \"It's okay, neither good nor bad.\"\n",
        "]\n",
        "\n",
        "# Preprocess the comments\n",
        "def text_processing(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'@\\w+|#\\w+|http\\S+', '', text)  # Remove mentions, hashtags, and URLs\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), \"\", text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "processed_comments = [text_processing(comment) for comment in new_comments]\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "sequences = tokenizer.texts_to_sequences(processed_comments)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100)  # Adjust maxlen as needed\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(padded_sequences)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Map class indices to labels\n",
        "sentiment_labels = []\n",
        "for pred in predicted_classes:\n",
        "    if pred == 0:\n",
        "        sentiment_labels.append('Negative')\n",
        "    elif pred == 1:\n",
        "        sentiment_labels.append('Neutral')\n",
        "    else:\n",
        "        sentiment_labels.append('Positive')\n",
        "\n",
        "# Print results\n",
        "for comment, label in zip(new_comments, sentiment_labels):\n",
        "    print(f\"Comment: {comment} - Sentiment: {label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDDlQIiBONEg",
        "outputId": "8aa8bcdf-6374-48d4-8e1c-4bbea0252b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 21s 21s/step\n",
            "Comment: I love this product! It's fantastic! - Sentiment: Positive\n",
            "Comment: This is the worst experience I've ever had. - Sentiment: Negative\n",
            "Comment: what about that. - Sentiment: Neutral\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load the model\n",
        "model = load_model('sentimentk.keras')\n",
        "\n",
        "# Load your tokenizer if you saved it\n",
        "# You can use joblib or pickle to load your tokenizer, for example:\n",
        "# import joblib\n",
        "# tokenizer = joblib.load('path_to_your_tokenizer.pkl')\n",
        "\n",
        "# Sample new comments for prediction\n",
        "new_comments = [\n",
        "    \"I love this product! It's fantastic!\",\n",
        "    \"This is the worst experience I've ever had.\",\n",
        "    \"what about that.\"\n",
        "]\n",
        "\n",
        "# Preprocess the comments\n",
        "def text_processing(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'@\\w+|#\\w+|http\\S+', '', text)  # Remove mentions, hashtags, and URLs\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), \"\", text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "processed_comments = [text_processing(comment) for comment in new_comments]\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "sequences = tokenizer.texts_to_sequences(processed_comments)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100)  # Adjust maxlen as needed\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(padded_sequences)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Map class indices to labels\n",
        "sentiment_labels = []\n",
        "for pred in predicted_classes:\n",
        "    if pred == 0:\n",
        "        sentiment_labels.append('Negative')\n",
        "    elif pred == 1:\n",
        "        sentiment_labels.append('Neutral')\n",
        "    else:\n",
        "        sentiment_labels.append('Positive')\n",
        "\n",
        "# Print results\n",
        "for comment, label in zip(new_comments, sentiment_labels):\n",
        "    print(f\"Comment: {comment} - Sentiment: {label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "z0BPB5y9tZOD"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Saving the tokenizer used in training\n",
        "with open('tokenizerk.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (tf_env)",
      "language": "python",
      "name": "tf_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
